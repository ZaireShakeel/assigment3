# -*- coding: utf-8 -*-
"""IRTM_Project_Assignmetn3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q-0M_K556nXSaKUP-b55-ntFpO3-CKYZ
"""

"""
Enhanced Information Retrieval System
Multi-field search using Article, Heading, Date, and NewsType
With Manual Relevance Judgments for Proper Evaluation
"""

import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
!pip install rank-bm25
from rank_bm25 import BM25Okapi
import time

# ============================================
# 1) LOAD DATA
# ============================================
print("Loading data...")
df = pd.read_csv("Articles.csv", encoding="ISO-8859-1")

# Fill missing values
df["Article"] = df["Article"].fillna("")
df["Heading"] = df["Heading"].fillna("")
df["NewsType"] = df["NewsType"].fillna("")
df["Date"] = df["Date"].fillna("")

print(f"Total documents: {len(df)}")
print(f"Columns: {df.columns.tolist()}")

# ============================================
# 2) TEXT CLEANING
# ============================================
def clean_text(text):
    """Clean and normalize text"""
    text = str(text).lower()
    text = re.sub(r"[^\w\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# Clean all text fields
df["clean_article"] = df["Article"].apply(clean_text)
df["clean_heading"] = df["Heading"].apply(clean_text)
df["clean_newstype"] = df["NewsType"].apply(clean_text)

# ============================================
# 3) MULTI-FIELD DOCUMENT REPRESENTATION
# ============================================
def create_weighted_document(row):
    """
    Combine multiple fields with weights:
    - Heading: 3x weight (most important for topic)
    - Article: 1x weight (main content)
    - NewsType: 2x weight (category context)
    """
    heading = " ".join([row["clean_heading"]] * 3)
    article = row["clean_article"]
    newstype = " ".join([row["clean_newstype"]] * 2)

    return f"{heading} {article} {newstype}".strip()

df["weighted_doc"] = df.apply(create_weighted_document, axis=1)
docs = df["weighted_doc"].tolist()

print(f"Created {len(docs)} weighted documents")

# ============================================
# 4) TF-IDF VECTORIZATION
# ============================================
print("\nBuilding TF-IDF index...")
vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2),  # Unigrams and bigrams
    min_df=2,  # Ignore very rare terms
    max_df=0.8  # Ignore very common terms
)
tfidf_matrix = vectorizer.fit_transform(docs)
print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")

# ============================================
# 5) BM25 SETUP
# ============================================
print("Building BM25 index...")
def tokenize(text):
    return text.split()

tokenized_docs = [tokenize(doc) for doc in docs]
bm25 = BM25Okapi(tokenized_docs)
print("BM25 index ready")

# ============================================
# 6) SEARCH FUNCTIONS
# ============================================

def tfidf_search(query, k=10):
    """TF-IDF based search using cosine similarity"""
    q_clean = clean_text(query)
    q_vec = vectorizer.transform([q_clean])
    scores = cosine_similarity(q_vec, tfidf_matrix).flatten()
    ranked = np.argsort(-scores)[:k]
    return ranked.tolist(), scores[ranked]

def bm25_search(query, k=10):
    """BM25 based probabilistic search"""
    q_clean = clean_text(query)
    q_tokens = tokenize(q_clean)
    scores = bm25.get_scores(q_tokens)
    ranked = np.argsort(-scores)[:k]
    return ranked.tolist(), scores[ranked]

def hybrid_search(query, alpha=0.6, k=10, boost_newstype=None):
    """
    Hybrid search combining BM25 and TF-IDF

    Parameters:
    - query: search query string
    - alpha: weight for BM25 (0-1), (1-alpha) weight for TF-IDF
    - k: number of results to return
    - boost_newstype: optional NewsType category to boost (e.g., "business")
    """
    q_clean = clean_text(query)

    # Get TF-IDF scores
    q_vec = vectorizer.transform([q_clean])
    tfidf_scores = cosine_similarity(q_vec, tfidf_matrix).flatten()

    # Get BM25 scores
    q_tokens = tokenize(q_clean)
    bm25_scores = np.array(bm25.get_scores(q_tokens))

    # Normalize scores to [0, 1]
    tfidf_norm = tfidf_scores / tfidf_scores.max() if tfidf_scores.max() > 0 else tfidf_scores
    bm25_norm = bm25_scores / bm25_scores.max() if bm25_scores.max() > 0 else bm25_scores

    # Combine scores with weights
    hybrid_scores = alpha * bm25_norm + (1 - alpha) * tfidf_norm

    # Optional: Boost specific NewsType category
    if boost_newstype:
        boost_newstype_clean = clean_text(boost_newstype)
        for i, row in df.iterrows():
            if boost_newstype_clean in row["clean_newstype"]:
                hybrid_scores[i] *= 1.2  # 20% boost

    # Rank documents by score
    ranked = np.argsort(-hybrid_scores)[:k]
    return ranked.tolist(), hybrid_scores[ranked]

# ============================================
# 7) DISPLAY RESULTS
# ============================================

def display_results(query, ranked_indices, scores, relevant_docs=None, method="Hybrid"):
    """Display search results in a readable format"""
    print(f"\n{'='*80}")
    print(f"Search Results for: '{query}' (Method: {method})")
    print(f"{'='*80}\n")

    for i, (idx, score) in enumerate(zip(ranked_indices, scores), 1):
        row = df.iloc[idx]
        relevant_mark = ""
        if relevant_docs and idx in relevant_docs:
            relevant_mark = " ✓ RELEVANT"

        print(f"Rank {i} | Doc ID: {idx} | Score: {score:.4f}{relevant_mark}")
        print(f"Heading: {row['Heading'][:80]}")
        print(f"NewsType: {row['NewsType']} | Date: {row['Date']}")
        print(f"Preview: {row['Article'][:150]}...")
        print(f"{'-'*80}\n")

# ============================================
# 8) EVALUATION METRICS
# ============================================

def precision_at_k(ranked, relevant, k=5):
    """Precision@K: What fraction of top-K results are relevant?"""
    retrieved_relevant = len([d for d in ranked[:k] if d in relevant])
    return retrieved_relevant / k

def recall_at_k(ranked, relevant, k=5):
    """Recall@K: What fraction of relevant docs are in top-K?"""
    if not relevant:
        return 0.0
    retrieved_relevant = len([d for d in ranked[:k] if d in relevant])
    return retrieved_relevant / len(relevant)

def f1_at_k(ranked, relevant, k=5):
    """F1@K: Harmonic mean of Precision@K and Recall@K"""
    p = precision_at_k(ranked, relevant, k)
    r = recall_at_k(ranked, relevant, k)
    if p + r == 0:
        return 0.0
    return 2 * (p * r) / (p + r)

def average_precision(ranked, relevant):
    """
    Average Precision (AP): Average of precision values at each relevant doc position
    Key metric for ranking quality
    """
    if not relevant:
        return 0.0
    hits = 0
    score = 0.0
    for i, doc_id in enumerate(ranked, 1):
        if doc_id in relevant:
            hits += 1
            score += hits / i
    return score / len(relevant)

def mean_reciprocal_rank(ranked, relevant):
    """MRR: Reciprocal rank of first relevant document"""
    for i, doc_id in enumerate(ranked, 1):
        if doc_id in relevant:
            return 1.0 / i
    return 0.0

def ndcg_at_k(ranked, relevant, k=5):
    """
    Normalized Discounted Cumulative Gain (NDCG@K)
    Measures ranking quality with position discount
    """
    dcg = 0.0
    for i, doc_id in enumerate(ranked[:k], 1):
        if doc_id in relevant:
            dcg += 1.0 / np.log2(i + 1)

    # Ideal DCG (best possible ranking)
    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))

    return dcg / idcg if idcg > 0 else 0.0

def evaluate_system(query, relevant_docs, method="hybrid", alpha=0.6, k=10):
    """
    Comprehensive evaluation of retrieval system
    Returns metrics and ranked results
    """
    print(f"\n{'='*80}")
    print(f"EVALUATION: Query = '{query}'")
    print(f"Method: {method.upper()}, Relevant docs: {len(relevant_docs)}")
    print(f"{'='*80}\n")

    # Measure query time
    start_time = time.time()

    if method == "tfidf":
        ranked, scores = tfidf_search(query, k=k)
    elif method == "bm25":
        ranked, scores = bm25_search(query, k=k)
    else:  # hybrid
        ranked, scores = hybrid_search(query, alpha=alpha, k=k)

    query_time = time.time() - start_time

    # Calculate all metrics
    metrics = {
        "Query Time (s)": round(query_time, 5),
        "P@5": precision_at_k(ranked, relevant_docs, k=5),
        "P@10": precision_at_k(ranked, relevant_docs, k=10),
        "R@5": recall_at_k(ranked, relevant_docs, k=5),
        "R@10": recall_at_k(ranked, relevant_docs, k=10),
        "F1@5": f1_at_k(ranked, relevant_docs, k=5),
        "F1@10": f1_at_k(ranked, relevant_docs, k=10),
        "MAP": average_precision(ranked, relevant_docs),
        "MRR": mean_reciprocal_rank(ranked, relevant_docs),
        "NDCG@5": ndcg_at_k(ranked, relevant_docs, k=5),
        "NDCG@10": ndcg_at_k(ranked, relevant_docs, k=10)
    }

    # Display metrics
    for metric, value in metrics.items():
        print(f"{metric:20s}: {value:.4f}")

    # Show which retrieved docs are relevant
    print(f"\nTop {min(10, len(ranked))} Retrieved Documents:")
    for i, idx in enumerate(ranked[:10], 1):
        relevant_mark = "✓ RELEVANT" if idx in relevant_docs else "✗ Not relevant"
        print(f"  {i}. Doc {idx:4d} - {relevant_mark}")

    return ranked, scores, metrics

# ============================================
# 9) MANUAL RELEVANCE JUDGMENTS
# ============================================
"""
IMPORTANT: These relevance judgments must be created manually
by reading documents and deciding which are relevant to each query.

Format:
test_queries = {
    "query_text": {doc_id1, doc_id2, doc_id3, ...},
    ...
}

Example workflow to create judgments:
1. Choose a query (e.g., "bengaluru")
2. Read multiple documents in the dataset
3. Decide which documents are truly relevant (even if they don't contain the exact term)
4. Add those document IDs to the set
"""

# Example: Manual relevance judgments (from your original code)
test_queries = {
    "bengaluru": {616, 1483, 1484, 1497, 1745},
    # Add more queries and their relevant documents here
   "surging": {4, 32, 37,48,64},
   "military": {569, 686, 711,732,783},
   "philanthropist": {2000,2446, 68},
   "finance Minister": {2437, 2450, 2463, 2482,2498},
    # "climate change": {100, 234, 456, 789},
}

# ============================================
# 10) MAIN EVALUATION
# ============================================

if __name__ == "__main__":

    print("\n" + "="*80)
    print("MULTI-FIELD INFORMATION RETRIEVAL SYSTEM")
    print("="*80)

    # For each test query, evaluate all methods
    for query, relevant_docs in test_queries.items():

        print("\n" + "="*80)
        print(f"QUERY: '{query}'")
        print(f"Number of manually judged relevant documents: {len(relevant_docs)}")
        print("="*80)

        methods_results = {}

        # Evaluate TF-IDF
        print("\n--- TF-IDF Method ---")
        _, _, metrics_tfidf = evaluate_system(query, relevant_docs, method="tfidf", k=10)
        methods_results["TF-IDF"] = metrics_tfidf

        # Evaluate BM25
        print("\n--- BM25 Method ---")
        _, _, metrics_bm25 = evaluate_system(query, relevant_docs, method="bm25", k=10)
        methods_results["BM25"] = metrics_bm25

        # Evaluate Hybrid (alpha=0.5)
        print("\n--- Hybrid Method (α=0.5) ---")
        _, _, metrics_hybrid_05 = evaluate_system(query, relevant_docs, method="hybrid", alpha=0.5, k=10)
        methods_results["Hybrid (α=0.5)"] = metrics_hybrid_05

        # Evaluate Hybrid (alpha=0.6)
        print("\n--- Hybrid Method (α=0.6) ---")
        ranked, scores, metrics_hybrid_06 = evaluate_system(query, relevant_docs, method="hybrid", alpha=0.6, k=10)
        methods_results["Hybrid (α=0.6)"] = metrics_hybrid_06

        # Comparison table
        print("\n" + "="*80)
        print("COMPARISON TABLE")
        print("="*80)
        print(f"{'Metric':<15} {'TF-IDF':<12} {'BM25':<12} {'Hybrid(0.5)':<12} {'Hybrid(0.6)':<12}")
        print("-"*80)

        key_metrics = ["P@5", "R@5", "F1@5", "MAP", "MRR", "NDCG@5"]
        for metric in key_metrics:
            print(f"{metric:<15} ", end="")
            for method in ["TF-IDF", "BM25", "Hybrid (α=0.5)", "Hybrid (α=0.6)"]:
                value = methods_results[method][metric]
                print(f"{value:<12.4f} ", end="")
            print()

        # Display detailed results for best method (Hybrid α=0.6)
        display_results(query, ranked[:5], scores[:5], relevant_docs, method="Hybrid (α=0.6)")

    print("\n" + "="*80)
    print("EVALUATION COMPLETE")
    print("="*80)
    print("\nTo add more test queries:")
    print("1. Choose a query relevant to your dataset")
    print("2. Manually read documents and judge relevance")
    print("3. Add to 'test_queries' dictionary with document IDs")
    print("="*80)

